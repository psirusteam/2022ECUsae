---
title: "Fundamentos de la inferencia Bayesiana en R y STAN"
subtitle: "CEPAL - División de Estadísticas Sociales"
author: "Andrés Gutiérrez - Stalyn Guerrero"
format: html
editor: visual
project:
  type: website
  output-dir: docs
---

```{r setup, include=FALSE, message=FALSE, error=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, cache = TRUE)
library(printr)
```

## Regla de Bayes

En términos de inferencia para $\boldsymbol{\theta}$, es necesario encontrar la distribución de los parámetros condicionada a la observación de los datos. Para este fin, es necesario definir la distribución conjunta de la variable de interés con el vector de parámetros.

$$
p(\boldsymbol{\theta},\mathbf{Y})=p(\boldsymbol{\theta})p(\mathbf{Y} \mid \boldsymbol{\theta})
$$

-   La distribución $p(\boldsymbol{\theta})$ se le conoce con el nombre de distribución previa.

-   El término $p(\mathbf{Y} \mid \boldsymbol{\theta})$ es la distribución de muestreo, verosimilitud o distribución de los datos.

-   La distribución del vector de parámetros condicionada a los datos observados está dada por

    $$
    p(\boldsymbol{\theta} \mid \mathbf{Y})=\frac{p(\boldsymbol{\theta},\mathbf{Y})}{p(\mathbf{Y})}=\frac{p(\boldsymbol{\theta})p(\mathbf{Y} \mid \boldsymbol{\theta})}{p(\mathbf{Y})}
    $$

-   A la distribución $p(\boldsymbol{\theta} \mid \mathbf{Y})$ se le conoce con el nombre de distribución ***posterior***. Nótese que el denominador no depende del vector de parámetros y considerando a los datos observados como fijos, corresponde a una constante y puede ser obviada. Por lo tanto, otra representación de la regla de Bayes está dada por

    $$
    p(\boldsymbol{\theta} \mid \mathbf{Y})\propto p(\mathbf{Y} \mid \boldsymbol{\theta})p(\boldsymbol{\theta})
    $$

## Inferencia Bayesiana.

En términos de estimación, inferencia y predicción, el enfoque Bayesiano supone dos momentos o etapas:

1.  **Antes de la recolección de las datos**, en donde el investigador propone, basado en su conocimiento, experiencia o fuentes externas, una distribución de probabilidad previa para el parámetro de interés.
2.  **Después de la recolección de los datos.** Siguiendo el teorema de Bayes, el investigador actualiza su conocimiento acerca del comportamiento probabilístico del parámetro de interés mediante la distribución posterior de este.

## Modelos uniparamétricos

Los modelos que están definidos en términos de un solo parámetro que pertenece al conjunto de los números reales se definen como modelos *uniparamétricos*.

## Modelo Bernoulli

Suponga que $Y$ es una variable aleatoria con distribución Bernoulli dada por:

$$
p(Y \mid \theta)=\theta^y(1-\theta)^{1-y}I_{\{0,1\}}(y)
$$

Como el parámetro $\theta$ está restringido al espacio $\Theta=[0,1]$, entonces es posible formular varias opciones para la distribución previa del parámetro. En particular, la distribución uniforme restringida al intervalo $[0,1]$ o la distribución Beta parecen ser buenas opciones. Puesto que la distribución uniforme es un caso particular de la distribución Beta. Por lo tanto la distribución previa del parámetro $\theta$ estará dada por

```{=tex}
\begin{equation}
p(\theta \mid \alpha,\beta)=
\frac{1}{Beta(\alpha,\beta)}\theta^{\alpha-1}(1-\theta)^{\beta-1}I_{[0,1]}(\theta).
\end{equation}
```
y la distribución posterior del parámetro $\theta$ sigue una distribución \begin{equation*}
\theta \mid Y \sim Beta(y+\alpha,\beta-y+1)
\end{equation*}

Cuando se tiene una muestra aleatoria $Y_1,\ldots,Y_n$ de variables con distribución Bernoulli de parámetro $\theta$, entonces la distribución posterior del parámetro de interés es

```{=tex}
\begin{equation*}
\theta \mid Y_1,\ldots,Y_n \sim Beta\left(\sum_{i=1}^ny_i+\alpha,\beta-\sum_{i=1}^ny_i+n\right)
\end{equation*}
```
### Práctica en **R**

-   Encuesta Nacional de Empleo, Desempleo y Subempleo (ENEMDU) 2020

La ENEMDU es una encuesta de aplicación continua, y la información generada de ella permite, además, identificar la magnitud de algunos fenómenos socio demográficos, al proporcionar datos e indicadores que muestran la situación en la que viven mujeres y hombres dentro de sus hogares y viviendas.

```{r, message=FALSE, echo=TRUE, warning=FALSE}
library(tidyverse)
encuesta <- readRDS("Data/encuestaECU20N.rds")  
```

Sea $Y$ la variable aleatoria $$
Y_{i}=\begin{cases}
1 & ingreso<lp\\
0 & ingreso\geq lp
\end{cases}
$$

El tamaño de la muestra es de $927$ Afrodescendiente.

```{r, message=FALSE, echo=TRUE, warning=FALSE}
datay <- encuesta %>% filter(etnia_ee == 2) %>% 
  transmute(y = ifelse(ingcorte < lp, 1,0))
addmargins(table(datay$y))
```

Un grupo de estadístico experto decide utilizar una distribución previa Beta, definiendo los parámetros de la distribución previa como $Beta(\alpha=350, \beta=650)$. La distribución posterior del parámetro de interés, que representa la probabilidad de estar por debajo de la linea de pobreza, es $Beta(347+350, 650-347+927)=Beta(697, 1230)$

```{r, BernoEj1, echo = FALSE, fig.cap="Distribuciones previas (línea punteada) y posteriores (línea sólida)"}
library(patchwork)
previa1 <- function(x)
  dbeta(x, 350, 650)
posterior1 <- function(x)
  dbeta(x, shape1 = 697, shape2 = 1230)

p1 <- ggplot(data = data.frame(x = 0),
             mapping = aes(x = x)) + ylab("f(x)") +
  stat_function(fun = previa1,
                mapping = aes(linetype = "solid"),
                size = 2) +
  stat_function(fun = posterior1,
                mapping = aes(linetype = "dashed"),
                size = 2) +
  xlim(0.3, 0.6) +
  theme(legend.position = "none")
p1
```

La estimación del parámetro estaría dado por: $$
E(X) = \frac{\alpha}{\alpha + \beta} = \frac{697}{697+1230} = 0.3617021
$$

luego, el intervalo de credibilidad para la distribución posterior es.

```{r, message=FALSE, echo=TRUE, warning=FALSE}
qbeta(c(0.025, 0.975),  697, 1230)
```

### Práctica en **STAN**

En `STAN` es posible obtener el mismo tipo de inferencia creando cuatro cadenas cuya distribución de probabilidad coincide con la distribución posterior del ejemplo.

```{r, eval=FALSE, results='markup'}
## Definir el modelo
data {                         // entradas del modelo 
  int<lower=0> n;              // número de observaciones  
  int y[n];                    // vector de longitud n
}
parameters {                   // definir parámetro
  real<lower=0, upper=1> theta;
}
model {                        // definir modelo
  y ~ bernoulli(theta);
  theta ~ beta(350, 650);      // distribución previa 
}
generated quantities {
    real ypred[n];                    // vector de longitud n
    for (ii in 1:n){
    ypred[ii] = bernoulli_rng(theta);
    }
}
```

Para compilar *STAN* debemos definir los parámetros de entrada

```{r}
    n <- nrow(datay)
    y <- datay$y
    sample_data <- list(n = n, y = y)
```

Para ejecutar `STAN` en R tenemos la librería *cmdstanr*

```{r, eval = TRUE, message=FALSE}
    library(cmdstanr)
Bernoulli <- cmdstan_model(stan_file = "Data/modelosStan/Bernoulli.stan")     
model_Bernoulli <- Bernoulli$sample(data = sample_data, 
                 chains = 4,
                 parallel_chains = 4,
                 seed = 1234,
                 refresh = 1000)
```

La estimación del parámetro $\theta$ es:

```{r}
model_Bernoulli$summary(variables = "theta")
```

Para observar las cadenas compilamos las lineas de código

```{r, fig.cap="Resultado con STAN (línea azul) y posterior teórica (línea negra)"}
library(posterior) 
library(ggplot2)
temp <- as_draws_df(model_Bernoulli$draws(variables = "theta"))
ggplot(data = temp, aes(x = theta))+ 
  geom_density(color = "blue", size = 2) +
  stat_function(fun = dbeta,
                args = list(shape1 = 697, shape2 = 1230),
                size = 2) + 
  theme_bw(base_size = 20)

```

Para validar las cadenas

```{r}
library(bayesplot)
(mcmc_dens_chains(model_Bernoulli$draws("theta")) +
mcmc_areas(model_Bernoulli$draws("theta")))/ 
mcmc_trace(model_Bernoulli$draws("theta")) 

```

Predicción de $Y$ en cada una de las iteraciones de las cadenas.

```{r, fig.cap="Resultado de la predicción para cada cadena (Puntos azules) y resultados en la muestra (Punto rojo)"}
n <- nrow(datay)
temp <- model_Bernoulli$draws(variables = "ypred", format = "df")

temp <- apply(temp, 1, 
      function(x){data.frame(
        (
          table(as.numeric(x[1:n]))))}) %>% 
  bind_rows()

ggplot(data = temp, aes(x = Var1, y = Freq))+ 
  geom_jitter(color = "blue", size = 2) +
  geom_point(data = data.frame((table(datay$y))),
             size = 3, color = "red")
 
```

```{r}
y_pred_B <- model_Bernoulli$draws(variables = "ypred", format = "matrix")
rowsrandom <- sample(nrow(y_pred_B), 100)
y_pred2 <- y_pred_B[rowsrandom, 1:n]
ppc_dens_overlay(y = datay$y, y_pred2) 
```

## Modelo Binomial

Cuando se dispone de una muestra aleatoria de variables con distribución Bernoulli $Y_1,\ldots,Y_n$, la inferencia Bayesiana se puede llevar a cabo usando la distribución Binomial, puesto que es bien sabido que la suma de variables aleatorias Bernoulli

```{=tex}
\begin{equation*}
S=\sum_{i=1}^nY_i
\end{equation*}
```
sigue una distribución Binomial. Es decir:

```{=tex}
\begin{equation}
p(S \mid \theta)=\binom{n}{s}\theta^s(1-\theta)^{n-s}I_{\{0,1,\ldots,n\}}(s),
\end{equation}
```
Nótese que la distribución binomial es un caso general para la distribución Bernoulli, cuando $n=1$. Por lo tanto es natural suponer que distribución previa del parámetro $\theta$ estará dada por

```{=tex}
\begin{equation}
p(\theta \mid \alpha,\beta)=
\frac{1}{Beta(\alpha,\beta)}\theta^{\alpha-1}(1-\theta)^{\beta-1}I_{[0,1]}(\theta).
\end{equation}
```
La distribución posterior del parámetro $\theta$ sigue una distribución

```{=tex}
\begin{equation*}
\theta \mid S \sim Beta(s+\alpha,\beta-s+n)
\end{equation*}
```
### Práctica en **STAN**

Sea $S$ el conteo de las personas en condición de pobreza en el país.

```{r, message=FALSE, echo=TRUE, warning=FALSE}
(dataS <- encuesta %>% 
  transmute(
  y = ifelse(ingcorte < lp, 1,0)
  ) %>%  
  summarise(n = n(),   #Número de ensayos 
            S = sum(y) #Número de éxito 
            ))
```

Creando código de `STAN`

```{r, eval=FALSE}
data {
  int<lower=0> n;              // Número de ensayos 
  int<lower=0> s;              // Número de éxitos
}
parameters {
  real<lower=0, upper=1> theta;     // theta|s1,s2..sD
}
model {

  s ~ binomial(n, theta);

  theta ~ beta(1600, 5000);

}

generated quantities {
    real spred;                    // vector de longitud D
    spred = binomial_rng(n, theta);

}

```

Preparando el código de `STAN`

```{r, eval=TRUE, results = ""}
## Definir el modelo
Binomial <- cmdstan_model(stan_file = "Data/modelosStan/Binomial.stan") 
```

Organizando datos para `STAN`

```{r}
sample_data <- list(s = dataS$S,
                    n = dataS$n)
```

Para ejecutar `STAN` en R tenemos la librería *cmdstanr*

```{r, eval = TRUE, message=FALSE}
model_Binomial <- Binomial$sample(data = sample_data, 
                 chains = 4,
                 parallel_chains = 4,
                 seed = 1234,
                 refresh = 1000)
```

La estimación del parámetro $\theta$ es:

```{r}
model_Binomial$summary(variables = "theta")
```

Para observar las cadenas compilamos las lineas de código

```{r, fig.cap="Resultado con STAN (línea azul) y distribución previa (línea negra)"}
temp <- model_Binomial$draws(variables = "theta", format = "df")
ggplot(data = temp, aes(x = theta))+ 
  geom_density(color = "blue", size = 2) +
  stat_function(fun = dbeta,
                args = list(shape1 = 1600, shape2 = 5000),
                size = 2) + 
  xlim(0.23,.27) +
  theme_bw(base_size = 20)

```

Para validar las cadenas

```{r}
(mcmc_dens_chains(model_Binomial$draws("theta")) +
mcmc_areas(model_Binomial$draws("theta")))/ 
mcmc_trace(model_Binomial$draws("theta")) 

```

```{r}
mcmc_trace(model_Binomial$draws("spred"))
```

Cuando se tiene una sucesión de variables aleatorias $S_1,\ldots,S_i, \ldots,S_k$ independientes y con distribución $Binomial(n_i,\theta)$ para $i=1,\ldots,k$, entonces la distribución posterior del parámetro de interés $\theta$ es

```{=tex}
\begin{equation*}
\theta \mid S_1,\ldots,S_k \sim Beta\left(\sum_{i=1}^ks_i+\alpha,\beta+\sum_{i=1}^k n_i-\sum_{i=1}^k s_i\right)
\end{equation*}
```
### Práctica en **STAN**

Sea $S_k$ el conteo de personas en condición de pobreza en la $k-ésima$ provincia en la muestra.

```{r, message=FALSE, echo=TRUE, warning=FALSE}
(dataS <- encuesta %>% 
  transmute(
  provincia = substr(upm,1,2),
  y = ifelse(ingcorte < lp, 1,0)
  ) %>% group_by(provincia) %>% 
  summarise(nd = n(),   #Número de ensayos 
            Sd = sum(y) #Número de éxito 
            ))
```

Creando código de `STAN`

```{r, eval=FALSE}
data {
  int<lower=0> K;                 // Número de provincia  
  int<lower=0> n[K];              // Número de ensayos 
  int<lower=0> s[K];              // Número de éxitos
}
parameters {
  real<lower=0, upper=1> theta; // theta|s1,s2,...sK
}
model {
  for(kk in 1:K) {
  s[kk] ~ binomial(n[kk], theta);
  }
  theta ~ beta(0.5, 0.5);
}

generated quantities {
    real spred[K];                    // vector de longitud D
    for(kk in 1:K){
    spred[kk] = binomial_rng(n[kk],theta);
}
}

```

Preparando el código de `STAN`

```{r, eval=TRUE, results = ""}
## Definir el modelo
Binomial2 <- cmdstan_model(stan_file = "Data/modelosStan/Binomial2.stan") 
```

Organizando datos para `STAN`

```{r}
sample_data <- list(K = nrow(dataS),
                    s = dataS$Sd,
                    n = dataS$nd)
```

Para ejecutar `STAN` en R tenemos la librería *cmdstanr*

```{r, eval = TRUE, message=FALSE}
model_Binomial2 <- Binomial2$sample(data = sample_data, 
                 chains = 4,
                 parallel_chains = 4,
                 seed = 1234,
                 refresh = 1000)
```

La estimación del parámetro $\theta$ es:

```{r}
model_Binomial2$summary(variables = "theta") %>% 
  data.frame()
```

Para validar las cadenas

```{r}
(mcmc_dens_chains(model_Binomial2$draws("theta")) +
mcmc_areas(model_Binomial2$draws("theta")))/ 
mcmc_trace(model_Binomial2$draws("theta")) 

```

```{r}
K <- nrow(dataS)
y_pred_B <- model_Binomial2$draws(variables = "spred", format = "matrix")
rowsrandom <- sample(nrow(y_pred_B), 200)
y_pred2 <- y_pred_B[rowsrandom, 1:K]
ppc_dens_overlay(y = dataS$Sd, y_pred2) 
```

## Modelo Poisson

Suponga que $\mathbf{Y}=\{Y_1,\ldots,Y_n\}$ es una muestra aleatoria de variables con distribución Poisson con parámetro $\theta$, la función de distribución conjunta o la función de verosimilitud está dada por

```{=tex}
\begin{align*}
p(\mathbf{Y} \mid \theta)&=\prod_{i=1}^n\frac{e^{-\theta}\theta^{y_i}}{y_i!}I_{\{0,1,\ldots\}}(y_i)\\
&=\frac{e^{-n\theta}\theta^{\sum_{i=1}^ny_i}}{\prod_{i=1}^ny_i!}I_{\{0,1,\ldots\}^n}(y_1,\ldots,y_n)
\end{align*}
```
donde $\{0,1\ldots\}^n$ denota el producto cartesiano $n$ veces sobre el conjunto $\{0,1\ldots\}$. Por otro lado, como el parámetro $\theta$ está restringido al espacio $\Theta=(0,\infty)$, entonces es posible formular varias opciones para la distribución previa del parámetro. Algunas de estas se encuentran considerando la distribución exponencial, la distribución Ji-cuadrado o la distribución Gamma. Sea la distribución previa del parámetro $\theta$ dada por

```{=tex}
\begin{equation}
p(\theta \mid \alpha,\beta)=\frac{\beta^\alpha}{\Gamma(\alpha)}\theta^{\alpha-1} e^{-\beta\theta}I_{(0,\infty)}(\theta).
\end{equation}
```
La distribución posterior del parámetro $\theta$ está dada por

```{=tex}
\begin{equation*}
\theta \mid \mathbf{Y} \sim Gamma\left(\sum_{i=1}^ny_i+\alpha,n+\beta\right)
\end{equation*}
```
### Práctica en **STAN**

Sea $Y$ el conteo de las personas por debajo de la linea de indigencia por división geográfica

```{r, fig.cap="Resultado en la muestra (puntos azules) y distribución teórica (puntos negros)"}
dataPois <- encuesta %>% 
  transmute(
  canton = substr(upm,1,6),
  y = ifelse(ingcorte < li, 1,0)
  ) %>% group_by(canton) %>% 
  summarise(yi = sum(y) #Número de éxito 
            )
(media <- mean(dataPois$yi))
temp <- dataPois %>%
  filter(yi>0) %>% group_by(yi) %>% tally() %>% 
  mutate(
  Porc = n/sum(n),
  Prob = dpois(yi,media))
temp

ggplot(temp,aes(x = yi, y = Porc))+ 
  geom_point( color = "blue") + xlim(0,30) +
  geom_point(data = temp,aes(x = yi, y = Prob),
            )
```

Creando código de `STAN`

```{r, eval=FALSE}
data {
  int<lower=0> n;      // Número de áreas geograficas 
  int<lower=0> y[n];   // Conteos por area
}
parameters {
  real<lower=0> theta;
}
model {
  y ~ poisson(theta);
  theta ~ gamma(38, 9);
}
generated quantities {
    real ypred[n];                    // vector de longitud n
    for(ii in 1:n){
    ypred[ii] = poisson_rng(theta);
    }
}

```

Preparando el código de `STAN`

```{r, eval=TRUE}
## Definir el modelo
Poisson <- cmdstan_model(stan_file = "Data/modelosStan/Poisson.stan") 
```

Organizando datos para `STAN`

```{r}
sample_data <- list(n = nrow(dataPois),
                    y = dataPois$yi)
```

Para ejecutar `STAN` en R tenemos la librería *cmdstanr*

```{r, eval = TRUE, message=FALSE}
model_Poisson <- Poisson$sample(data = sample_data, 
                 chains = 4,
                 parallel_chains = 4,
                 seed = 1234,
                 refresh = 1000)
```

La estimación del parámetro $\theta$ es:

```{r}
model_Poisson$summary(variables = "theta")
```

```{r}
(mcmc_dens_chains(model_Poisson$draws("theta")) +
mcmc_areas(model_Poisson$draws("theta")))/ 
mcmc_trace(model_Poisson$draws("theta")) 

```

```{r}
n <- nrow(dataPois)
y_pred_B <- model_Poisson$draws(variables = "ypred", format = "matrix")
rowsrandom <- sample(nrow(y_pred_B), 11)
y_pred2 <- y_pred_B[rowsrandom, 1:n]
ppc_hist(y = dataPois$yi, y_pred2) +
 xlim(0,30) 
```

## Modelo Normal con media desconocida

Suponga que $Y_1,\cdots,Y_n$ son variables independientes e idénticamente distribuidos con distribución $Normal(\theta,\sigma^2)$ con $\theta$ desconocido pero $\sigma^2$ conocido. De esta forma, la función de verosimilitud de los datos está dada por

$$
\begin{align*}
p(\mathbf{Y} \mid \theta)
&=\prod_{i=1}^n\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left\{-\frac{1}{2\sigma^2}(y_i-\theta)^2\right\}I_\mathbb{R}(y) \\
&=(2\pi\sigma^2)^{-n/2}\exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-\theta)^2\right\}
\end{align*}
$$

Como el parámetro $\theta$ puede tomar cualquier valor en los reales, es posible asignarle una distribución previa $\theta \sim Normal(\mu,\tau^2)$. Bajo este marco de referencia se tienen los siguientes resultados

La distribución posterior del parámetro de interés $\theta$ sigue una distribución

```{=tex}
\begin{equation*}
\theta|\mathbf{Y} \sim Normal(\mu_n,\tau^2_n).
\end{equation*}
```
En donde

```{=tex}
\begin{equation}
(\#eq:TauSigman)
\mu_n=\frac{\frac{n}{\sigma^2}\bar{Y}+\frac{1}{\tau^2}\mu}{\frac{n}{\sigma^2}+\frac{1}{\tau^2}}
\ \ \ \ \ \ \ \text{y} \ \ \ \ \ \ \
\tau_n^2=\left(\frac{n}{\sigma^2}+\frac{1}{\tau^2}\right)^{-1}
\end{equation}
```
### Práctica en **STAN**

Sea $Y$ el logaritmo del ingreso

```{r, fig.cap="Resultado en la muestra (línea azul) y distribución teórica (línea negra)"}
dataNormal <- encuesta %>%
    transmute(
      provincia = substr(upm,1,2),
      logIngreso = log(ingcorte +1)) %>% 
  filter(provincia == "01")

(media <- mean(dataNormal$logIngreso))
(Sd <- sd(dataNormal$logIngreso))

ggplot(dataNormal,aes(x = logIngreso))+ 
  geom_density(size =2, color = "blue") +
  stat_function(fun =dnorm, 
                args = list(mean = media, sd = Sd),
                size =2) +
  theme_bw(base_size = 20)
```

Creando código de `STAN`

```{r, eval=FALSE}
data {
  int<lower=0> n;     // Número de observaciones
  real y[n];          // LogIngreso 
  real <lower=0> Sigma;  // Desviación estándar   
}
parameters {
  real theta;
}
model {
  y ~ normal(theta, Sigma);
  theta ~ normal(0, 20); // distribución previa
}
generated quantities {
    real ypred[n];                    // vector de longitud D
    for(kk in 1:n){
    ypred[kk] = normal_rng(theta,Sigma);
}
}


```

Preparando el código de `STAN`

```{r, eval=TRUE}
NormalMedia <- cmdstan_model(stan_file = "Data/modelosStan/NormalMedia.stan") 
```

Organizando datos para `STAN`

```{r}
sample_data <- list(n = nrow(dataNormal),
                    Sigma = sd(dataNormal$logIngreso),
                    y = dataNormal$logIngreso)
```

Para ejecutar `STAN` en R tenemos la librería *cmdstanr*

```{r, eval = TRUE, message=FALSE}
model_NormalMedia <- NormalMedia$sample(data = sample_data, 
                 chains = 4,
                 parallel_chains = 4,
                 seed = 1234,
                 refresh = 1000
                 )
```

La estimación del parámetro $\theta$ es:

```{r}
model_NormalMedia$summary(variables = "theta")
```

```{r}
(mcmc_dens_chains(model_NormalMedia$draws("theta")) +
mcmc_areas(model_NormalMedia$draws("theta")))/ 
mcmc_trace(model_NormalMedia$draws("theta")) 

```

```{r}
n <- nrow(dataNormal)
y_pred_B <- model_NormalMedia$draws(variables = "ypred", format = "matrix")
rowsrandom <- sample(nrow(y_pred_B), 100)
y_pred2 <- y_pred_B[rowsrandom, 1:n]
ppc_dens_overlay(y = as.numeric(dataNormal$logIngreso), y_pred2)
```

# Modelos multiparamétricos

-   La distribución normal univariada que tiene dos parámetros: la media $\theta$ y la varianza $\sigma^2$.
-   La distribución multinomial cuyo parámetro es un vector de probabilidades $\boldsymbol{\theta}$.

## Modelo Normal con media y varianza desconocida

Supongamos que se dispone de realizaciones de un conjunto de variables independientes e idénticamente distribuidas $Y_1,\cdots,Y_n\sim N(\theta,\sigma^2)$. Cuando se desconoce tanto la media como la varianza de la distribución es necesario plantear diversos enfoques y situarse en el más conveniente, según el contexto del problema. En términos de la asignación de las distribuciones previas para $\theta$ y $\sigma^2$ es posible:

-   Suponer que la distribución previa $p(\theta)$ es independiente de la distribución previa $p(\sigma^2)$ y que ambas distribuciones son informativas.
-   Suponer que la distribución previa $p(\theta)$ es independiente de la distribución previa $p(\sigma^2)$ y que ambas distribuciones son no informativas.
-   Suponer que la distribución previa para $\theta$ depende de $\sigma^2$ y escribirla como $p(\theta \mid \sigma^2)$, mientras que la distribución previa de $\sigma^2$ no depende de $\theta$ y se puede escribir como $p(\sigma^2)$.

## Parámetros independientes

La distribución previa para el parámetro $\theta$ será

```{=tex}
\begin{equation*}
\theta \sim Normal(\mu,\tau^2)
\end{equation*}
```
Y la distribución previa para el parámetro $\sigma^2$ será

```{=tex}
\begin{equation*}
\sigma^2 \sim Inversa-Gamma(n_0/2,n_0\sigma^2_0/2)
\end{equation*}
```
Asumiendo independencia previa, la distribución previa conjunta estará dada por

```{=tex}
\begin{equation}
p(\theta,\sigma^2)\propto (\sigma^2)^{-n_0/2-1}\exp\left\{-\dfrac{n_0\sigma^2_0}{2\sigma^2}\right\}
\exp\left\{-\frac{1}{2\tau^2}(\theta-\mu)^2\right\}
\end{equation}
```
La distribución posterior conjunta de los parámetros de interés está dada por

```{=tex}
\begin{align}
p(\theta,\sigma^2 \mid \mathbf{Y})&\propto (\sigma^2)^{-(n+n_0)/2-1} \notag \\
&\times
\exp\left\{-\frac{1}{2\sigma^2}\left[n_0\sigma^2_0+(n-1)S^2+n(\bar{y}-\theta)^2\right]-\frac{1}{2\tau^2}(\theta-\mu)^2\right\}
\end{align}
```
La distribución posterior condicional de $\theta$ es

```{=tex}
\begin{equation}
\theta  \mid  \sigma^2,\mathbf{Y} \sim Normal(\mu_n,\tau_n^2)
\end{equation}
```
En donde las expresiones para $\mu_n$ y $\tau_n^2$ están dadas por \@ref(eq:TauSigman). Por otro lado, la distribución posterior condicional de $\sigma^2$ es

```{=tex}
\begin{equation}
\sigma^2  \mid  \theta,\mathbf{Y} \sim Inversa-Gamma\left(\dfrac{n_0+n}{2},\dfrac{v_0}{2}\right)
\end{equation}
```
con $v_0=n_0\sigma^2_0+(n-1)S^2+n(\bar{y}-\theta)^2$.

### Práctica en **STAN**

Sea $Y$ el logaritmo del ingreso

```{r}
dataNormal <- encuesta %>%
    transmute(
      provincia = substr(upm,1,2),
      logIngreso = log(ingcorte +1)) %>% 
  filter(provincia == "02")
```

Creando código de `STAN`

```{r, eval=FALSE}
data {
  int<lower=0> n;
  real y[n];
}
parameters {
  real sigma;
  real theta;
}
transformed parameters {
  real sigma2;
  sigma2 = pow(sigma, 2);
}
model {
  y ~ normal(theta, sigma);
  theta ~ normal(0, 1000);
  sigma2 ~ inv_gamma(0.001, 0.001);
}
generated quantities {
    real ypred[n];                    // vector de longitud D
    for(kk in 1:n){
    ypred[kk] = normal_rng(theta,sigma);
}
}

```

Preparando el código de `STAN`

```{r, eval=TRUE}
NormalMeanVar  <- cmdstan_model(stan_file = "Data/modelosStan/NormalMeanVar.stan") 
```

Organizando datos para `STAN`

```{r}
sample_data <- list(n = nrow(dataNormal),
                    y = dataNormal$logIngreso)
```

Para ejecutar `STAN` en R tenemos la librería *cmdstanr*

```{r, eval = TRUE, message=FALSE}
model_NormalMedia <- NormalMeanVar$sample(data = sample_data, 
                 chains = 4,
                 parallel_chains = 4,
                 seed = 1234,
                 refresh = 1000)
```

La estimación del parámetro $\theta$ es:

```{r}
model_NormalMedia$summary(variables = c("theta", "sigma", "sigma2"))
```

```{r}
(mcmc_dens_chains(model_NormalMedia$draws("theta")) +
mcmc_areas(model_NormalMedia$draws("theta")))/ 
mcmc_trace(model_NormalMedia$draws("theta")) 

```

```{r}
(mcmc_dens_chains(model_NormalMedia$draws("sigma")) +
mcmc_areas(model_NormalMedia$draws("sigma")))/ 
mcmc_trace(model_NormalMedia$draws("sigma")) 

```

```{r}
(mcmc_dens_chains(model_NormalMedia$draws("sigma2")) +
mcmc_areas(model_NormalMedia$draws("sigma2")))/ 
mcmc_trace(model_NormalMedia$draws("sigma2")) 

```

```{r}
n <- nrow(dataNormal)
y_pred_B <- model_NormalMedia$draws(variables = "ypred", format = "matrix")
rowsrandom <- sample(nrow(y_pred_B), 100)
y_pred2 <- y_pred_B[rowsrandom, 1:n]
ppc_dens_overlay(y = as.numeric(dataNormal$logIngreso), y_pred2)
```

## Modelo Multinomial

En esta sección discutimos el modelamiento bayesiano de datos provenientes de una distribución multinomial que corresponde a una extensión multivariada de la distribución binomial. Suponga que $\textbf{Y}=(Y_1,\ldots,Y_p)'$ es un vector aleatorio con distribución multinomial, así, su distribución está parametrizada por el vector $\boldsymbol{\theta}=(\theta_1,\ldots,\theta_p)'$ y está dada por la siguiente expresión

```{=tex}
\begin{equation}
p(\mathbf{Y} \mid \boldsymbol{\theta})=\binom{n}{y_1,\ldots,y_p}\prod_{i=1}^p\theta_i^{y_i} \ \ \ \ \ \theta_i>0 \texttt{ , }  \sum_{i=1}^py_i=n \texttt{ y } \sum_{i=1}^p\theta_i=1
\end{equation}
```
Donde

```{=tex}
\begin{equation*}
\binom{n}{y_1,\ldots,y_p}=\frac{n!}{y_1!\cdots y_p!}.
\end{equation*}
```
Como cada parámetro $\theta_i$ está restringido al espacio $\Theta=[0,1]$, entonces es posible asignar a la distribución de Dirichlet como la distribución previa del vector de parámetros. Por lo tanto la distribución previa del vector de parámetros $\boldsymbol{\theta}$, parametrizada por el vector de hiperparámetros $\boldsymbol{\alpha}=(\alpha_1,\ldots,\alpha_p)'$, está dada por

```{=tex}
\begin{equation}
p(\boldsymbol{\theta} \mid \boldsymbol{\alpha})=\frac{\Gamma(\alpha_1+\cdots+\alpha_p)}{\Gamma(\alpha_1)\cdots\Gamma(\alpha_p)}
  \prod_{i=1}^p\theta_i^{\alpha_i-1} \ \ \ \ \ \alpha_i>0 \texttt{ y } \sum_{i=1}^p\theta_i=1
\end{equation}
```
La distribución posterior del parámetro $\boldsymbol{\theta}$ sigue una distribución $Dirichlet(y_1+\alpha_1,\ldots,y_p+\alpha_p)$

### Práctica en **STAN**

Sea $Y$ Condición de actividad laboral

```{r}
(dataMult <- encuesta %>% filter(condact3>0) %>% 
  transmute(empleo = as_factor(condact3)) %>% 
  group_by(empleo) %>%  tally())
```

Creando código de `STAN`

```{r, eval=FALSE}
data {
  int<lower=0> k;  // Número de cátegoria 
  int y[k];        // Número de exitos 
  vector[k] alpha; // Parámetro de las distribción previa 
}
parameters {
  simplex[k] theta;
}
transformed parameters {
  real delta;
}
model {
  y ~ multinomial(theta);
  theta ~ dirichlet(alpha);
}
generated quantities {
  int ypred[k];
  ypred = multinomial_rng(theta, 100);
}


```

Preparando el código de `STAN`

```{r, eval=TRUE}
Multinom  <- cmdstan_model(stan_file = "Data/modelosStan/Multinom.stan") 
```

Organizando datos para `STAN`

```{r}
sample_data <- list(k = nrow(dataMult),
                    y = dataMult$n,
                    alpha = c(1000, 1000,1000))
```

Para ejecutar `STAN` en R tenemos la librería *cmdstanr*

```{r, eval = TRUE, message=FALSE}
model_Multinom <- Multinom$sample(data = sample_data, 
                 chains = 4,
                 parallel_chains = 4,
                 seed = 1234,
                 refresh = 1000)
```

La estimación del parámetro $\theta$ es:

```{r}
model_Multinom$summary(variables = c("theta"))
```

```{r}
(mcmc_dens_chains(model_Multinom$draws("theta[1]")) +
mcmc_areas(model_Multinom$draws("theta[1]")))/ 
mcmc_trace(model_Multinom$draws("theta[1]")) 

```

```{r}
(mcmc_dens_chains(model_Multinom$draws("theta[2]")) +
mcmc_areas(model_Multinom$draws("theta[2]")))/ 
mcmc_trace(model_Multinom$draws("theta[2]")) 

```

```{r}
(mcmc_dens_chains(model_Multinom$draws("theta[3]")) +
mcmc_areas(model_Multinom$draws("theta[3]")))/ 
mcmc_trace(model_Multinom$draws("theta[3]")) 

```
